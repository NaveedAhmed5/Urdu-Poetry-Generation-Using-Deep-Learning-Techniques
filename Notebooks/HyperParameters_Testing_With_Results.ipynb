{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. Uninstall conflicting packages to clean the slate\n!pip uninstall -y numpy pandas matplotlib pyarrow datasets\n\n# 2. Reinstall compatible versions together\n# We stick to numpy<2 for safety, but reinstalling dependent libs ensures they link correctly.\n!pip install \"numpy<2\" pandas matplotlib pyarrow datasets torch\n\n# 3. CRITICAL: Restart the Runtime/Kernel now!\n# In Colab: Runtime > Restart Session\n# In Jupyter: Kernel > Restart","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:29:05.257150Z","iopub.execute_input":"2025-11-30T11:29:05.258120Z","iopub.status.idle":"2025-11-30T11:29:24.115735Z","shell.execute_reply.started":"2025-11-30T11:29:05.258088Z","shell.execute_reply":"2025-11-30T11:29:24.114960Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: pandas 2.3.3\nUninstalling pandas-2.3.3:\n  Successfully uninstalled pandas-2.3.3\nFound existing installation: matplotlib 3.10.7\nUninstalling matplotlib-3.10.7:\n  Successfully uninstalled matplotlib-3.10.7\nFound existing installation: pyarrow 22.0.0\nUninstalling pyarrow-22.0.0:\n  Successfully uninstalled pyarrow-22.0.0\nFound existing installation: datasets 4.4.1\nUninstalling datasets-4.4.1:\n  Successfully uninstalled datasets-4.4.1\nCollecting numpy<2\n  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting pandas\n  Using cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\nCollecting matplotlib\n  Using cached matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nCollecting pyarrow\n  Using cached pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nCollecting datasets\n  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nUsing cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\nUsing cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\nUsing cached matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\nUsing cached pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\nUsing cached datasets-4.4.1-py3-none-any.whl (511 kB)\nInstalling collected packages: pyarrow, numpy, pandas, matplotlib, datasets\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-4.4.1 matplotlib-3.10.7 numpy-1.26.4 pandas-2.3.3 pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Hyperparameter Experiments\nStandalone notebook to run OFAT experiments and generate results CSV.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport math\nimport os\nimport re\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\n# Set device\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {DEVICE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:29:24.117540Z","iopub.execute_input":"2025-11-30T11:29:24.117793Z","iopub.status.idle":"2025-11-30T11:29:24.124326Z","shell.execute_reply.started":"2025-11-30T11:29:24.117768Z","shell.execute_reply":"2025-11-30T11:29:24.123638Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class UrduPoetryTokenizer:\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.vocab_size = 0\n\n    def fit_on_texts(self, texts):\n        all_words = []\n        for text in texts:\n            words = self._clean_and_tokenize(text)\n            all_words.extend(words)\n\n        word_counts = Counter(all_words)\n        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n\n        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\", 2: \"<SOS>\", 3: \"<EOS>\"}\n\n        idx = 4\n        for word, _ in sorted_words:\n            if word not in self.word2idx:\n                self.word2idx[word] = idx\n                self.idx2word[idx] = word\n                idx += 1\n\n        self.vocab_size = len(self.word2idx)\n        print(f\"Vocabulary size: {self.vocab_size}\")\n\n    def _clean_and_tokenize(self, text):\n        text = str(text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text.split(' ')\n\n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            words = self._clean_and_tokenize(text)\n            seq = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in words]\n            sequences.append(seq)\n        return sequences\n\n    def sequences_to_texts(self, sequences):\n        texts = []\n        for seq in sequences:\n            words = [self.idx2word.get(idx, \"<UNK>\") for idx in seq]\n            words = [w for w in words if w not in [\"<PAD>\", \"<SOS>\", \"<EOS>\"]]\n            texts.append(\" \".join(words))\n        return texts\n\ndef load_and_process_data(max_seq_len=20, batch_size=128):\n    print(\"Loading dataset...\")\n    try:\n        dataset = load_dataset(\"ReySajju742/Urdu-Poetry-Dataset\")\n        data_split = dataset['train'] if 'train' in dataset else dataset['test']\n\n        all_lines = []\n        for item in data_split:\n            content = item.get('content', '') or item.get('Poem', '') or list(item.values())[0]\n            if content:\n                lines = content.split('\\n')\n                all_lines.extend([l for l in lines if l.strip()])\n\n        print(f\"Total lines extracted: {len(all_lines)}\")\n\n        tokenizer = UrduPoetryTokenizer()\n        tokenizer.fit_on_texts(all_lines)\n\n        sequences = tokenizer.texts_to_sequences(all_lines)\n\n        input_sequences = []\n        for seq in sequences:\n            for i in range(1, len(seq)):\n                n_gram_seq = seq[:i+1]\n                if len(n_gram_seq) <= max_seq_len + 1:\n                    input_sequences.append(n_gram_seq)\n                else:\n                    input_sequences.append(n_gram_seq[-(max_seq_len+1):])\n\n        print(f\"Total sequences created: {len(input_sequences)}\")\n\n        padded_sequences = []\n        for seq in input_sequences:\n            pad_len = (max_seq_len + 1) - len(seq)\n            padded_seq = [0] * pad_len + seq\n            padded_sequences.append(padded_seq)\n\n        padded_sequences = np.array(padded_sequences)\n\n        X = padded_sequences[:, :-1]\n        y = padded_sequences[:, -1]\n\n        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.long))\n        val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.long), torch.tensor(y_val, dtype=torch.long))\n        test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long), torch.tensor(y_test, dtype=torch.long))\n\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n        return train_loader, val_loader, test_loader, tokenizer\n\n    except Exception as e:\n        print(f\"Error in data loading: {e}\")\n        return None, None, None, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:29:24.125293Z","iopub.execute_input":"2025-11-30T11:29:24.125648Z","iopub.status.idle":"2025-11-30T11:29:24.145168Z","shell.execute_reply.started":"2025-11-30T11:29:24.125618Z","shell.execute_reply":"2025-11-30T11:29:24.144402Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class SimpleRNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.2):\n        super(SimpleRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x, hidden=None):\n        embeds = self.embedding(x)\n        output, hidden = self.rnn(embeds, hidden)\n        prediction = self.fc(output)\n        return prediction, hidden\n\nclass LSTMModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.2):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x, hidden=None):\n        embeds = self.embedding(x)\n        output, hidden = self.lstm(embeds, hidden)\n        prediction = self.fc(output)\n        return prediction, hidden\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\nclass TransformerModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers=2, dropout=0.2, max_len=100):\n        super(TransformerModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.pos_encoder = PositionalEncoding(embed_dim, max_len)\n\n        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n\n        self.fc = nn.Linear(embed_dim, vocab_size)\n        self.embed_dim = embed_dim\n\n    def forward(self, x, mask=None):\n        embeds = self.embedding(x) * math.sqrt(self.embed_dim)\n        embeds = self.pos_encoder(embeds)\n\n        if mask is None:\n            seq_len = x.size(1)\n            mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\n            mask = mask.to(x.device)\n\n        output = self.transformer_encoder(embeds, mask=mask, is_causal=True)\n        prediction = self.fc(output)\n        return prediction, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:29:24.146059Z","iopub.execute_input":"2025-11-30T11:29:24.146418Z","iopub.status.idle":"2025-11-30T11:29:24.167119Z","shell.execute_reply.started":"2025-11-30T11:29:24.146391Z","shell.execute_reply":"2025-11-30T11:29:24.166300Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_optimizer(model, optimizer_name, learning_rate):\n    if optimizer_name.lower() == 'adam':\n        return optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer_name.lower() == 'rmsprop':\n        return optim.RMSprop(model.parameters(), lr=learning_rate)\n    elif optimizer_name.lower() == 'sgd':\n        return optim.SGD(model.parameters(), lr=learning_rate)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\ndef train_epoch(model, dataloader, criterion, optimizer, device, clip=1.0):\n    model.train()\n    total_loss = 0\n\n    for batch in dataloader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n\n        if isinstance(model, (nn.RNN, nn.LSTM, nn.GRU)):\n            outputs, _ = model(inputs)\n            output = outputs[:, -1, :]\n        else:\n            outputs, _ = model(inputs)\n            output = outputs[:, -1, :]\n\n        loss = criterion(output, targets)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            if isinstance(model, (nn.RNN, nn.LSTM, nn.GRU)):\n                outputs, _ = model(inputs)\n                output = outputs[:, -1, :]\n            else:\n                outputs, _ = model(inputs)\n                output = outputs[:, -1, :]\n\n            loss = criterion(output, targets)\n            total_loss += loss.item()\n\n            _, predicted = torch.max(output, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n    return total_loss / len(dataloader), correct / total\n\ndef train_model(model, train_loader, val_loader, optimizer_name, learning_rate, epochs, device, patience=5):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = get_optimizer(model, optimizer_name, learning_rate)\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_ppl': [],\n        'val_ppl': [],\n        'val_acc': []\n    }\n\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n\n        train_ppl = math.exp(train_loss)\n        val_ppl = math.exp(val_loss)\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_ppl'].append(train_ppl)\n        history['val_ppl'].append(val_ppl)\n        history['val_acc'].append(val_acc)\n\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val PPL: {val_ppl:.4f} | Val Acc: {val_acc:.4f}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n                break\n\n    total_time = time.time() - start_time\n    return history, total_time\n\ndef generate_text(model, tokenizer, seed_text, max_length=20, temperature=1.0, device='cpu'):\n    model.eval()\n    words = tokenizer._clean_and_tokenize(seed_text)\n    current_seq = [tokenizer.word2idx.get(w, tokenizer.word2idx[\"<UNK>\"]) for w in words]\n\n    generated_words = []\n\n    with torch.no_grad():\n        for _ in range(max_length):\n            inp = torch.tensor([current_seq], dtype=torch.long).to(device)\n            if inp.size(1) > 100:\n                 inp = inp[:, -100:]\n\n            if isinstance(model, (torch.nn.RNN, torch.nn.LSTM, torch.nn.GRU)):\n                outputs, _ = model(inp)\n                output = outputs[:, -1, :]\n            else:\n                outputs, _ = model(inp)\n                output = outputs[:, -1, :]\n\n            output = output / temperature\n            probs = F.softmax(output, dim=-1)\n            next_token_idx = torch.multinomial(probs, 1).item()\n\n            current_seq.append(next_token_idx)\n            word = tokenizer.idx2word.get(next_token_idx, \"<UNK>\")\n            generated_words.append(word)\n\n            if word == \"<EOS>\":\n                break\n\n    return seed_text + \" \" + \" \".join(generated_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:29:24.168617Z","iopub.execute_input":"2025-11-30T11:29:24.168857Z","iopub.status.idle":"2025-11-30T11:29:24.191720Z","shell.execute_reply.started":"2025-11-30T11:29:24.168843Z","shell.execute_reply":"2025-11-30T11:29:24.190993Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Experiment Runner\nresults_list = []\n\ndef run_experiment(exp_id, param_name, param_value, model_type='LSTM', optimizer_name='RMSprop', \n                   baseline_ppl=None, epochs=20, batch_size=128, seq_len=20, \n                   layers=2, dropout=0.2, heads=4, ff_dim=512, blocks=2):\n    \n    print(f\"Running {exp_id}: {param_name}={param_value} (Model={model_type}, Opt={optimizer_name})\")\n    \n    # Reload data if needed\n    if param_name in ['batch_size', 'sequence_length']:\n        t_loader, v_loader, _, _ = load_and_process_data(max_seq_len=seq_len, batch_size=batch_size)\n    else:\n        t_loader, v_loader = train_loader, val_loader\n\n    # Model Setup\n    if model_type == 'RNN':\n        model = SimpleRNN(vocab_size, EMBED_DIM, HIDDEN_DIM, num_layers=layers, dropout=dropout).to(DEVICE)\n    elif model_type == 'LSTM':\n        model = LSTMModel(vocab_size, EMBED_DIM, HIDDEN_DIM, num_layers=layers, dropout=dropout).to(DEVICE)\n    elif model_type == 'Transformer':\n        model = TransformerModel(vocab_size, EMBED_DIM, num_heads=heads, hidden_dim=ff_dim, num_layers=blocks, dropout=dropout, max_len=seq_len+1).to(DEVICE)\n\n    # Optimizer Setup\n    lr = 0.001 # Default\n    if param_name == 'learning_rate': lr = param_value\n    elif optimizer_name == 'SGD': lr = 0.01\n    elif optimizer_name == 'Adam' and model_type == 'Transformer': lr = 0.0001\n\n    # Train\n    history, train_time = train_model(model, t_loader, v_loader, optimizer_name, lr, epochs, DEVICE, patience=5)\n    \n    # Metrics\n    min_val_loss = min(history['val_loss'])\n    exp_ppl = math.exp(min_val_loss)\n    \n    change = 0\n    if baseline_ppl:\n        change = baseline_ppl - exp_ppl # Positive means improvement (lower PPL)\n\n    result_entry = {\n        'Experiment': exp_id,\n        'Parameter': param_name,\n        'Value': param_value,\n        'Model': model_type,\n        'Optimizer': optimizer_name,\n        'Perplexity': round(exp_ppl, 2),\n        'Change': round(change, 2) if baseline_ppl else '-',\n        'Best?': '' \n    }\n    \n    results_list.append(result_entry)\n    return exp_ppl\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:29:24.192344Z","iopub.execute_input":"2025-11-30T11:29:24.192542Z","iopub.status.idle":"2025-11-30T11:29:24.205864Z","shell.execute_reply.started":"2025-11-30T11:29:24.192527Z","shell.execute_reply":"2025-11-30T11:29:24.205135Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Execution of Experiments\n\n# --- 5.1.1 Architecture (LSTM Baseline) ---\nprint(\"--- Architecture Experiments ---\")\nbaseline_lstm = run_experiment(\"Baseline\", \"-\", \"-\", model_type='LSTM', optimizer_name='RMSprop')\n\n# Layers\nfor l in [1, 3]:\n    run_experiment(f\"EXP-LAYERS-{l}\", \"num_layers\", l, model_type='LSTM', baseline_ppl=baseline_lstm, layers=l)\n\n# Dropout\nfor d in [0.1, 0.3, 0.5]:\n    run_experiment(f\"EXP-DROPOUT-{d}\", \"dropout\", d, model_type='LSTM', baseline_ppl=baseline_lstm, dropout=d)\n\n# --- 5.1.2 Training ---\nprint(\"\\n--- Training Experiments ---\")\n# Learning Rate\nfor lr in [0.0001, 0.01, 0.1]:\n    run_experiment(f\"EXP-LR-{lr}\", \"learning_rate\", lr, model_type='LSTM', baseline_ppl=baseline_lstm)\n\n# Batch Size\nfor bs in [32, 64, 256]:\n    run_experiment(f\"EXP-BATCH-{bs}\", \"batch_size\", bs, model_type='LSTM', baseline_ppl=baseline_lstm, batch_size=bs)\n\n# Epochs\nfor ep in [10, 30, 50]:\n    run_experiment(f\"EXP-EPOCHS-{ep}\", \"epochs\", ep, model_type='LSTM', baseline_ppl=baseline_lstm, epochs=ep)\n\n# --- 5.1.3 Transformer ---\nprint(\"\\n--- Transformer Experiments ---\")\nbaseline_trans = run_experiment(\"Baseline-Trans\", \"-\", \"-\", model_type='Transformer', optimizer_name='Adam')\n\n# Heads\nfor h in [2, 8]:\n    run_experiment(f\"EXP-HEADS-{h}\", \"num_heads\", h, model_type='Transformer', optimizer_name='Adam', baseline_ppl=baseline_trans, heads=h)\n\n# FF Dim\nfor ff in [256, 1024]:\n    run_experiment(f\"EXP-FF-{ff}\", \"feedforward_dim\", ff, model_type='Transformer', optimizer_name='Adam', baseline_ppl=baseline_trans, ff_dim=ff)\n\n# Blocks\nfor b in [1, 3, 4]:\n    run_experiment(f\"EXP-BLOCKS-{b}\", \"blocks\", b, model_type='Transformer', optimizer_name='Adam', baseline_ppl=baseline_trans, blocks=b)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:29:24.206727Z","iopub.execute_input":"2025-11-30T11:29:24.207678Z","iopub.status.idle":"2025-11-30T12:37:37.463504Z","shell.execute_reply.started":"2025-11-30T11:29:24.207657Z","shell.execute_reply":"2025-11-30T12:37:37.462505Z"}},"outputs":[{"name":"stdout","text":"--- Architecture Experiments ---\nRunning Baseline: -=- (Model=LSTM, Opt=RMSprop)\nEpoch 1/20 | Train Loss: 6.6600 | Val Loss: 6.3932 | Val PPL: 597.7842 | Val Acc: 0.0831\nEpoch 2/20 | Train Loss: 6.1136 | Val Loss: 6.2592 | Val PPL: 522.7974 | Val Acc: 0.1027\nEpoch 3/20 | Train Loss: 5.6949 | Val Loss: 6.2501 | Val PPL: 518.0887 | Val Acc: 0.1120\nEpoch 4/20 | Train Loss: 5.2362 | Val Loss: 6.3291 | Val PPL: 560.6322 | Val Acc: 0.1132\nEpoch 5/20 | Train Loss: 4.7510 | Val Loss: 6.5153 | Val PPL: 675.4258 | Val Acc: 0.1127\nEpoch 6/20 | Train Loss: 4.2456 | Val Loss: 6.7348 | Val PPL: 841.1845 | Val Acc: 0.1127\nEpoch 7/20 | Train Loss: 3.7586 | Val Loss: 6.9961 | Val PPL: 1092.3374 | Val Acc: 0.1121\nEpoch 8/20 | Train Loss: 3.3057 | Val Loss: 7.2522 | Val PPL: 1411.2572 | Val Acc: 0.1024\nEarly stopping triggered after 8 epochs.\nRunning EXP-LAYERS-1: num_layers=1 (Model=LSTM, Opt=RMSprop)\nEpoch 1/20 | Train Loss: 6.5236 | Val Loss: 6.2753 | Val PPL: 531.3005 | Val Acc: 0.0984\nEpoch 2/20 | Train Loss: 5.6839 | Val Loss: 6.2408 | Val PPL: 513.2894 | Val Acc: 0.1134\nEpoch 3/20 | Train Loss: 4.9559 | Val Loss: 6.3270 | Val PPL: 559.4655 | Val Acc: 0.1160\nEpoch 4/20 | Train Loss: 4.2394 | Val Loss: 6.4839 | Val PPL: 654.5206 | Val Acc: 0.1139\nEpoch 5/20 | Train Loss: 3.5502 | Val Loss: 6.6876 | Val PPL: 802.3909 | Val Acc: 0.1122\nEpoch 6/20 | Train Loss: 2.9189 | Val Loss: 6.9191 | Val PPL: 1011.3638 | Val Acc: 0.1073\nEpoch 7/20 | Train Loss: 2.3701 | Val Loss: 7.1824 | Val PPL: 1316.0563 | Val Acc: 0.1035\nEarly stopping triggered after 7 epochs.\nRunning EXP-LAYERS-3: num_layers=3 (Model=LSTM, Opt=RMSprop)\nEpoch 1/20 | Train Loss: 6.7937 | Val Loss: 6.5873 | Val PPL: 725.8279 | Val Acc: 0.0644\nEpoch 2/20 | Train Loss: 6.4051 | Val Loss: 6.4452 | Val PPL: 629.6966 | Val Acc: 0.0795\nEpoch 3/20 | Train Loss: 6.1143 | Val Loss: 6.3839 | Val PPL: 592.2069 | Val Acc: 0.0915\nEpoch 4/20 | Train Loss: 5.8282 | Val Loss: 6.3830 | Val PPL: 591.6983 | Val Acc: 0.0934\nEpoch 5/20 | Train Loss: 5.5414 | Val Loss: 6.4258 | Val PPL: 617.5682 | Val Acc: 0.1000\nEpoch 6/20 | Train Loss: 5.2231 | Val Loss: 6.5475 | Val PPL: 697.4674 | Val Acc: 0.1013\nEpoch 7/20 | Train Loss: 4.9073 | Val Loss: 6.7792 | Val PPL: 879.3861 | Val Acc: 0.0992\nEpoch 8/20 | Train Loss: 4.5888 | Val Loss: 6.9875 | Val PPL: 1083.0451 | Val Acc: 0.0991\nEpoch 9/20 | Train Loss: 4.2758 | Val Loss: 7.2077 | Val PPL: 1349.7806 | Val Acc: 0.0987\nEarly stopping triggered after 9 epochs.\nRunning EXP-DROPOUT-0.1: dropout=0.1 (Model=LSTM, Opt=RMSprop)\nEpoch 1/20 | Train Loss: 6.6674 | Val Loss: 6.3973 | Val PPL: 600.2422 | Val Acc: 0.0864\nEpoch 2/20 | Train Loss: 6.1177 | Val Loss: 6.2903 | Val PPL: 539.3318 | Val Acc: 0.1007\nEpoch 3/20 | Train Loss: 5.6815 | Val Loss: 6.2610 | Val PPL: 523.7448 | Val Acc: 0.1119\nEpoch 4/20 | Train Loss: 5.1850 | Val Loss: 6.3393 | Val PPL: 566.3829 | Val Acc: 0.1147\nEpoch 5/20 | Train Loss: 4.6408 | Val Loss: 6.5830 | Val PPL: 722.6820 | Val Acc: 0.1127\nEpoch 6/20 | Train Loss: 4.0703 | Val Loss: 6.8204 | Val PPL: 916.3409 | Val Acc: 0.1117\nEpoch 7/20 | Train Loss: 3.5059 | Val Loss: 7.1175 | Val PPL: 1233.4216 | Val Acc: 0.1052\nEpoch 8/20 | Train Loss: 2.9977 | Val Loss: 7.3831 | Val PPL: 1608.5426 | Val Acc: 0.1052\nEarly stopping triggered after 8 epochs.\nRunning EXP-DROPOUT-0.3: dropout=0.3 (Model=LSTM, Opt=RMSprop)\nEpoch 1/20 | Train Loss: 6.6899 | Val Loss: 6.4138 | Val PPL: 610.2341 | Val Acc: 0.0837\nEpoch 2/20 | Train Loss: 6.1615 | Val Loss: 6.3007 | Val PPL: 544.9535 | Val Acc: 0.1017\nEpoch 3/20 | Train Loss: 5.7664 | Val Loss: 6.2684 | Val PPL: 527.6124 | Val Acc: 0.1094\nEpoch 4/20 | Train Loss: 5.3445 | Val Loss: 6.3213 | Val PPL: 556.2718 | Val Acc: 0.1100\nEpoch 5/20 | Train Loss: 4.9173 | Val Loss: 6.4546 | Val PPL: 635.6359 | Val Acc: 0.1161\nEpoch 6/20 | Train Loss: 4.4891 | Val Loss: 6.6366 | Val PPL: 762.5111 | Val Acc: 0.1140\nEpoch 7/20 | Train Loss: 4.0773 | Val Loss: 6.8453 | Val PPL: 939.4805 | Val Acc: 0.1125\nEpoch 8/20 | Train Loss: 3.6918 | Val Loss: 7.0217 | Val PPL: 1120.6490 | Val Acc: 0.1067\nEarly stopping triggered after 8 epochs.\nRunning EXP-DROPOUT-0.5: dropout=0.5 (Model=LSTM, Opt=RMSprop)\nEpoch 1/20 | Train Loss: 6.6880 | Val Loss: 6.4308 | Val PPL: 620.6413 | Val Acc: 0.0806\nEpoch 2/20 | Train Loss: 6.1919 | Val Loss: 6.3259 | Val PPL: 558.8391 | Val Acc: 0.0982\nEpoch 3/20 | Train Loss: 5.8533 | Val Loss: 6.2753 | Val PPL: 531.2931 | Val Acc: 0.1071\nEpoch 4/20 | Train Loss: 5.5312 | Val Loss: 6.2907 | Val PPL: 539.5078 | Val Acc: 0.1147\nEpoch 5/20 | Train Loss: 5.2236 | Val Loss: 6.3575 | Val PPL: 576.8090 | Val Acc: 0.1135\nEpoch 6/20 | Train Loss: 4.9218 | Val Loss: 6.4855 | Val PPL: 655.5353 | Val Acc: 0.1169\nEpoch 7/20 | Train Loss: 4.6229 | Val Loss: 6.6210 | Val PPL: 750.6700 | Val Acc: 0.1116\nEpoch 8/20 | Train Loss: 4.3545 | Val Loss: 6.7612 | Val PPL: 863.6792 | Val Acc: 0.1117\nEarly stopping triggered after 8 epochs.\n\n--- Training Experiments ---\nRunning EXP-LR-0.0001: learning_rate=0.0001 (Model=LSTM, Opt=RMSprop)\nEpoch 1/20 | Train Loss: 6.8735 | Val Loss: 6.7692 | Val PPL: 870.6207 | Val Acc: 0.0482\nEpoch 2/20 | Train Loss: 6.6075 | Val Loss: 6.7107 | Val PPL: 821.1749 | Val Acc: 0.0645\nEpoch 3/20 | Train Loss: 6.4599 | Val Loss: 6.6364 | Val PPL: 762.3368 | Val Acc: 0.0745\nEpoch 4/20 | Train Loss: 6.3266 | Val Loss: 6.5773 | Val PPL: 718.5943 | Val Acc: 0.0783\nEpoch 5/20 | Train Loss: 6.2037 | Val Loss: 6.5242 | Val PPL: 681.4502 | Val Acc: 0.0828\nEpoch 6/20 | Train Loss: 6.0904 | Val Loss: 6.4721 | Val PPL: 646.8202 | Val Acc: 0.0870\nEpoch 7/20 | Train Loss: 5.9827 | Val Loss: 6.4433 | Val PPL: 628.4854 | Val Acc: 0.0883\nEpoch 8/20 | Train Loss: 5.8788 | Val Loss: 6.4191 | Val PPL: 613.4358 | Val Acc: 0.0920\nEpoch 9/20 | Train Loss: 5.7772 | Val Loss: 6.3916 | Val PPL: 596.8084 | Val Acc: 0.0955\nEpoch 10/20 | Train Loss: 5.6758 | Val Loss: 6.3863 | Val PPL: 593.6625 | Val Acc: 0.0960\nEpoch 11/20 | Train Loss: 5.5749 | Val Loss: 6.3830 | Val PPL: 591.6812 | Val Acc: 0.0987\nEpoch 12/20 | Train Loss: 5.4743 | Val Loss: 6.3980 | Val PPL: 600.6483 | Val Acc: 0.0999\nEpoch 13/20 | Train Loss: 5.3729 | Val Loss: 6.3729 | Val PPL: 585.7493 | Val Acc: 0.1000\nEpoch 14/20 | Train Loss: 5.2723 | Val Loss: 6.3919 | Val PPL: 597.0012 | Val Acc: 0.1001\nEpoch 15/20 | Train Loss: 5.1717 | Val Loss: 6.3879 | Val PPL: 594.6066 | Val Acc: 0.1017\nEpoch 16/20 | Train Loss: 5.0735 | Val Loss: 6.4265 | Val PPL: 617.9831 | Val Acc: 0.1010\nEpoch 17/20 | Train Loss: 4.9770 | Val Loss: 6.4118 | Val PPL: 609.0168 | Val Acc: 0.1030\nEpoch 18/20 | Train Loss: 4.8801 | Val Loss: 6.4442 | Val PPL: 629.0470 | Val Acc: 0.1016\nEarly stopping triggered after 18 epochs.\nRunning EXP-LR-0.01: learning_rate=0.01 (Model=LSTM, Opt=RMSprop)\nEpoch 1/20 | Train Loss: 7.2686 | Val Loss: 6.9177 | Val PPL: 1010.0156 | Val Acc: 0.0466\nEpoch 2/20 | Train Loss: 6.8689 | Val Loss: 6.9210 | Val PPL: 1013.2886 | Val Acc: 0.0529\nEpoch 3/20 | Train Loss: 6.7228 | Val Loss: 6.9381 | Val PPL: 1030.8403 | Val Acc: 0.0608\nEpoch 4/20 | Train Loss: 6.5959 | Val Loss: 6.8880 | Val PPL: 980.4789 | Val Acc: 0.0630\nEpoch 5/20 | Train Loss: 6.4136 | Val Loss: 6.9656 | Val PPL: 1059.5494 | Val Acc: 0.0702\nEpoch 6/20 | Train Loss: 6.3131 | Val Loss: 7.0965 | Val PPL: 1207.7352 | Val Acc: 0.0719\nEpoch 7/20 | Train Loss: 6.2607 | Val Loss: 7.2197 | Val PPL: 1366.0933 | Val Acc: 0.0737\nEpoch 8/20 | Train Loss: 6.2260 | Val Loss: 7.2363 | Val PPL: 1388.9547 | Val Acc: 0.0741\nEpoch 9/20 | Train Loss: 6.1832 | Val Loss: 7.3122 | Val PPL: 1498.4713 | Val Acc: 0.0756\nEarly stopping triggered after 9 epochs.\nRunning EXP-LR-0.1: learning_rate=0.1 (Model=LSTM, Opt=RMSprop)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1754885686.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Learning Rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"EXP-LR-{lr}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_ppl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Batch Size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1910242761.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(exp_id, param_name, param_value, model_type, optimizer_name, baseline_ppl, epochs, batch_size, seq_len, layers, dropout, heads, ff_dim, blocks)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2489811609.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer_name, learning_rate, epochs, device, patience)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mtrain_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mval_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOverflowError\u001b[0m: math range error"],"ename":"OverflowError","evalue":"math range error","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"# Batch Size\nfor bs in [32, 64, 256]:\n    run_experiment(f\"EXP-BATCH-{bs}\", \"batch_size\", bs, model_type='LSTM', baseline_ppl=baseline_lstm, batch_size=bs)\n\n# Epochs\nfor ep in [10, 30, 50]:\n    run_experiment(f\"EXP-EPOCHS-{ep}\", \"epochs\", ep, model_type='LSTM', baseline_ppl=baseline_lstm, epochs=ep)\n\n# --- 5.1.3 Transformer ---\nprint(\"\\n--- Transformer Experiments ---\")\nbaseline_trans = run_experiment(\"Baseline-Trans\", \"-\", \"-\", model_type='Transformer', optimizer_name='Adam')\n\n# Heads\nfor h in [2, 8]:\n    run_experiment(f\"EXP-HEADS-{h}\", \"num_heads\", h, model_type='Transformer', optimizer_name='Adam', baseline_ppl=baseline_trans, heads=h)\n\n# FF Dim\nfor ff in [256, 1024]:\n    run_experiment(f\"EXP-FF-{ff}\", \"feedforward_dim\", ff, model_type='Transformer', optimizer_name='Adam', baseline_ppl=baseline_trans, ff_dim=ff)\n\n# Blocks\nfor b in [1, 3, 4]:\n    run_experiment(f\"EXP-BLOCKS-{b}\", \"blocks\", b, model_type='Transformer', optimizer_name='Adam', baseline_ppl=baseline_trans, blocks=b)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:49:48.631055Z","iopub.execute_input":"2025-11-30T12:49:48.631448Z","iopub.status.idle":"2025-11-30T14:19:05.375646Z","shell.execute_reply.started":"2025-11-30T12:49:48.631426Z","shell.execute_reply":"2025-11-30T14:19:05.374904Z"}},"outputs":[{"name":"stdout","text":"Running EXP-BATCH-32: batch_size=32 (Model=LSTM, Opt=RMSprop)\nLoading dataset...\nTotal lines extracted: 21077\nVocabulary size: 10520\nTotal sequences created: 152146\nEpoch 1/20 | Train Loss: 6.6329 | Val Loss: 6.3974 | Val PPL: 600.2525 | Val Acc: 0.0900\nEpoch 2/20 | Train Loss: 6.1964 | Val Loss: 6.2783 | Val PPL: 532.8846 | Val Acc: 0.1050\nEpoch 3/20 | Train Loss: 5.9458 | Val Loss: 6.3251 | Val PPL: 558.4350 | Val Acc: 0.1128\nEpoch 4/20 | Train Loss: 5.7373 | Val Loss: 6.5121 | Val PPL: 673.2054 | Val Acc: 0.1108\nEpoch 5/20 | Train Loss: 5.5026 | Val Loss: 6.6880 | Val PPL: 802.6783 | Val Acc: 0.1118\nEpoch 6/20 | Train Loss: 5.2028 | Val Loss: 6.9527 | Val PPL: 1045.9308 | Val Acc: 0.1082\nEpoch 7/20 | Train Loss: 4.8850 | Val Loss: 7.2489 | Val PPL: 1406.5058 | Val Acc: 0.1050\nEarly stopping triggered after 7 epochs.\nRunning EXP-BATCH-64: batch_size=64 (Model=LSTM, Opt=RMSprop)\nLoading dataset...\nTotal lines extracted: 21077\nVocabulary size: 10520\nTotal sequences created: 152146\nEpoch 1/20 | Train Loss: 6.6457 | Val Loss: 6.3939 | Val PPL: 598.1822 | Val Acc: 0.0884\nEpoch 2/20 | Train Loss: 6.1483 | Val Loss: 6.2684 | Val PPL: 527.6411 | Val Acc: 0.1017\nEpoch 3/20 | Train Loss: 5.7708 | Val Loss: 6.2645 | Val PPL: 525.5690 | Val Acc: 0.1113\nEpoch 4/20 | Train Loss: 5.4000 | Val Loss: 6.3854 | Val PPL: 593.1369 | Val Acc: 0.1155\nEpoch 5/20 | Train Loss: 5.0266 | Val Loss: 6.5428 | Val PPL: 694.2345 | Val Acc: 0.1131\nEpoch 6/20 | Train Loss: 4.6544 | Val Loss: 6.8097 | Val PPL: 906.6397 | Val Acc: 0.1113\nEpoch 7/20 | Train Loss: 4.2830 | Val Loss: 7.0567 | Val PPL: 1160.6299 | Val Acc: 0.1063\nEpoch 8/20 | Train Loss: 3.9219 | Val Loss: 7.2979 | Val PPL: 1477.2210 | Val Acc: 0.1055\nEarly stopping triggered after 8 epochs.\nRunning EXP-BATCH-256: batch_size=256 (Model=LSTM, Opt=RMSprop)\nLoading dataset...\nTotal lines extracted: 21077\nVocabulary size: 10520\nTotal sequences created: 152146\nEpoch 1/20 | Train Loss: 6.6690 | Val Loss: 6.4014 | Val PPL: 602.6796 | Val Acc: 0.0814\nEpoch 2/20 | Train Loss: 6.1001 | Val Loss: 6.2881 | Val PPL: 538.1056 | Val Acc: 0.0980\nEpoch 3/20 | Train Loss: 5.6582 | Val Loss: 6.2812 | Val PPL: 534.4535 | Val Acc: 0.1095\nEpoch 4/20 | Train Loss: 5.1895 | Val Loss: 6.3644 | Val PPL: 580.8185 | Val Acc: 0.1130\nEpoch 5/20 | Train Loss: 4.6851 | Val Loss: 6.4885 | Val PPL: 657.5187 | Val Acc: 0.1165\nEpoch 6/20 | Train Loss: 4.1816 | Val Loss: 6.6902 | Val PPL: 804.4777 | Val Acc: 0.1108\nEpoch 7/20 | Train Loss: 3.7117 | Val Loss: 6.8724 | Val PPL: 965.2401 | Val Acc: 0.1100\nEpoch 8/20 | Train Loss: 3.2820 | Val Loss: 7.0837 | Val PPL: 1192.3146 | Val Acc: 0.1055\nEarly stopping triggered after 8 epochs.\nRunning EXP-EPOCHS-10: epochs=10 (Model=LSTM, Opt=RMSprop)\nEpoch 1/10 | Train Loss: 6.6218 | Val Loss: 6.3522 | Val PPL: 573.7796 | Val Acc: 0.0881\nEpoch 2/10 | Train Loss: 6.0487 | Val Loss: 6.2522 | Val PPL: 519.1420 | Val Acc: 0.1067\nEpoch 3/10 | Train Loss: 5.5882 | Val Loss: 6.2524 | Val PPL: 519.2640 | Val Acc: 0.1113\nEpoch 4/10 | Train Loss: 5.0864 | Val Loss: 6.3602 | Val PPL: 578.3544 | Val Acc: 0.1142\nEpoch 5/10 | Train Loss: 4.5693 | Val Loss: 6.5640 | Val PPL: 709.0990 | Val Acc: 0.1134\nEpoch 6/10 | Train Loss: 4.0466 | Val Loss: 6.8143 | Val PPL: 910.7887 | Val Acc: 0.1110\nEpoch 7/10 | Train Loss: 3.5556 | Val Loss: 7.0261 | Val PPL: 1125.6087 | Val Acc: 0.1077\nEarly stopping triggered after 7 epochs.\nRunning EXP-EPOCHS-30: epochs=30 (Model=LSTM, Opt=RMSprop)\nEpoch 1/30 | Train Loss: 6.6510 | Val Loss: 6.3768 | Val PPL: 588.0715 | Val Acc: 0.0889\nEpoch 2/30 | Train Loss: 6.0856 | Val Loss: 6.2726 | Val PPL: 529.8623 | Val Acc: 0.1062\nEpoch 3/30 | Train Loss: 5.6490 | Val Loss: 6.2552 | Val PPL: 520.6962 | Val Acc: 0.1125\nEpoch 4/30 | Train Loss: 5.1717 | Val Loss: 6.3579 | Val PPL: 577.0174 | Val Acc: 0.1107\nEpoch 5/30 | Train Loss: 4.6699 | Val Loss: 6.5316 | Val PPL: 686.4954 | Val Acc: 0.1142\nEpoch 6/30 | Train Loss: 4.1601 | Val Loss: 6.8081 | Val PPL: 905.1127 | Val Acc: 0.1083\nEpoch 7/30 | Train Loss: 3.6815 | Val Loss: 7.0275 | Val PPL: 1127.2473 | Val Acc: 0.1075\nEpoch 8/30 | Train Loss: 3.2376 | Val Loss: 7.2671 | Val PPL: 1432.4039 | Val Acc: 0.1059\nEarly stopping triggered after 8 epochs.\nRunning EXP-EPOCHS-50: epochs=50 (Model=LSTM, Opt=RMSprop)\nEpoch 1/50 | Train Loss: 6.6799 | Val Loss: 6.4197 | Val PPL: 613.7905 | Val Acc: 0.0838\nEpoch 2/50 | Train Loss: 6.1309 | Val Loss: 6.2837 | Val PPL: 535.7722 | Val Acc: 0.1014\nEpoch 3/50 | Train Loss: 5.7032 | Val Loss: 6.2449 | Val PPL: 515.3665 | Val Acc: 0.1090\nEpoch 4/50 | Train Loss: 5.2285 | Val Loss: 6.3336 | Val PPL: 563.1928 | Val Acc: 0.1128\nEpoch 5/50 | Train Loss: 4.7362 | Val Loss: 6.5157 | Val PPL: 675.6559 | Val Acc: 0.1086\nEpoch 6/50 | Train Loss: 4.2375 | Val Loss: 6.7513 | Val PPL: 855.2099 | Val Acc: 0.1125\nEpoch 7/50 | Train Loss: 3.7507 | Val Loss: 6.9725 | Val PPL: 1066.9167 | Val Acc: 0.1042\nEpoch 8/50 | Train Loss: 3.3044 | Val Loss: 7.1886 | Val PPL: 1324.2427 | Val Acc: 0.1030\nEarly stopping triggered after 8 epochs.\n\n--- Transformer Experiments ---\nRunning Baseline-Trans: -=- (Model=Transformer, Opt=Adam)\nEpoch 1/20 | Train Loss: 7.0023 | Val Loss: 6.7183 | Val PPL: 827.4043 | Val Acc: 0.0567\nEpoch 2/20 | Train Loss: 6.5664 | Val Loss: 6.5990 | Val PPL: 734.3941 | Val Acc: 0.0626\nEpoch 3/20 | Train Loss: 6.3656 | Val Loss: 6.5002 | Val PPL: 665.2471 | Val Acc: 0.0777\nEpoch 4/20 | Train Loss: 6.1871 | Val Loss: 6.4430 | Val PPL: 628.2611 | Val Acc: 0.0824\nEpoch 5/20 | Train Loss: 6.0302 | Val Loss: 6.3997 | Val PPL: 601.6874 | Val Acc: 0.0879\nEpoch 6/20 | Train Loss: 5.8818 | Val Loss: 6.3687 | Val PPL: 583.2982 | Val Acc: 0.0925\nEpoch 7/20 | Train Loss: 5.7434 | Val Loss: 6.3510 | Val PPL: 573.0729 | Val Acc: 0.0954\nEpoch 8/20 | Train Loss: 5.6146 | Val Loss: 6.3452 | Val PPL: 569.7546 | Val Acc: 0.0998\nEpoch 9/20 | Train Loss: 5.4914 | Val Loss: 6.3472 | Val PPL: 570.8674 | Val Acc: 0.1011\nEpoch 10/20 | Train Loss: 5.3751 | Val Loss: 6.3567 | Val PPL: 576.3679 | Val Acc: 0.1021\nEpoch 11/20 | Train Loss: 5.2658 | Val Loss: 6.3713 | Val PPL: 584.8077 | Val Acc: 0.1029\nEpoch 12/20 | Train Loss: 5.1625 | Val Loss: 6.3866 | Val PPL: 593.8359 | Val Acc: 0.1061\nEpoch 13/20 | Train Loss: 5.0557 | Val Loss: 6.4075 | Val PPL: 606.3949 | Val Acc: 0.1052\nEarly stopping triggered after 13 epochs.\nRunning EXP-HEADS-2: num_heads=2 (Model=Transformer, Opt=Adam)\nEpoch 1/20 | Train Loss: 7.0030 | Val Loss: 6.7043 | Val PPL: 815.9454 | Val Acc: 0.0548\nEpoch 2/20 | Train Loss: 6.5547 | Val Loss: 6.5880 | Val PPL: 726.3147 | Val Acc: 0.0703\nEpoch 3/20 | Train Loss: 6.3550 | Val Loss: 6.5024 | Val PPL: 666.7264 | Val Acc: 0.0779\nEpoch 4/20 | Train Loss: 6.1778 | Val Loss: 6.4387 | Val PPL: 625.5682 | Val Acc: 0.0843\nEpoch 5/20 | Train Loss: 6.0165 | Val Loss: 6.3949 | Val PPL: 598.7628 | Val Acc: 0.0891\nEpoch 6/20 | Train Loss: 5.8663 | Val Loss: 6.3711 | Val PPL: 584.7172 | Val Acc: 0.0918\nEpoch 7/20 | Train Loss: 5.7308 | Val Loss: 6.3535 | Val PPL: 574.5257 | Val Acc: 0.0950\nEpoch 8/20 | Train Loss: 5.6045 | Val Loss: 6.3450 | Val PPL: 569.6540 | Val Acc: 0.0968\nEpoch 9/20 | Train Loss: 5.4829 | Val Loss: 6.3503 | Val PPL: 572.6769 | Val Acc: 0.0993\nEpoch 10/20 | Train Loss: 5.3717 | Val Loss: 6.3715 | Val PPL: 584.9087 | Val Acc: 0.1004\nEpoch 11/20 | Train Loss: 5.2648 | Val Loss: 6.3703 | Val PPL: 584.2612 | Val Acc: 0.1042\nEpoch 12/20 | Train Loss: 5.1614 | Val Loss: 6.3986 | Val PPL: 601.0165 | Val Acc: 0.1044\nEpoch 13/20 | Train Loss: 5.0616 | Val Loss: 6.4146 | Val PPL: 610.7183 | Val Acc: 0.1036\nEarly stopping triggered after 13 epochs.\nRunning EXP-HEADS-8: num_heads=8 (Model=Transformer, Opt=Adam)\nEpoch 1/20 | Train Loss: 6.9947 | Val Loss: 6.7025 | Val PPL: 814.4660 | Val Acc: 0.0563\nEpoch 2/20 | Train Loss: 6.5395 | Val Loss: 6.5647 | Val PPL: 709.6197 | Val Acc: 0.0721\nEpoch 3/20 | Train Loss: 6.3417 | Val Loss: 6.4900 | Val PPL: 658.5000 | Val Acc: 0.0784\nEpoch 4/20 | Train Loss: 6.1656 | Val Loss: 6.4312 | Val PPL: 620.8926 | Val Acc: 0.0829\nEpoch 5/20 | Train Loss: 6.0080 | Val Loss: 6.3880 | Val PPL: 594.6841 | Val Acc: 0.0861\nEpoch 6/20 | Train Loss: 5.8606 | Val Loss: 6.3621 | Val PPL: 579.4595 | Val Acc: 0.0914\nEpoch 7/20 | Train Loss: 5.7197 | Val Loss: 6.3490 | Val PPL: 571.9126 | Val Acc: 0.0934\nEpoch 8/20 | Train Loss: 5.5843 | Val Loss: 6.3401 | Val PPL: 566.8415 | Val Acc: 0.0962\nEpoch 9/20 | Train Loss: 5.4619 | Val Loss: 6.3464 | Val PPL: 570.4409 | Val Acc: 0.0994\nEpoch 10/20 | Train Loss: 5.3384 | Val Loss: 6.3594 | Val PPL: 577.8775 | Val Acc: 0.1012\nEpoch 11/20 | Train Loss: 5.2277 | Val Loss: 6.3739 | Val PPL: 586.3576 | Val Acc: 0.1013\nEpoch 12/20 | Train Loss: 5.1161 | Val Loss: 6.3902 | Val PPL: 595.9935 | Val Acc: 0.1008\nEpoch 13/20 | Train Loss: 5.0131 | Val Loss: 6.4205 | Val PPL: 614.2903 | Val Acc: 0.1021\nEarly stopping triggered after 13 epochs.\nRunning EXP-FF-256: feedforward_dim=256 (Model=Transformer, Opt=Adam)\nEpoch 1/20 | Train Loss: 7.0166 | Val Loss: 6.7349 | Val PPL: 841.2409 | Val Acc: 0.0560\nEpoch 2/20 | Train Loss: 6.5904 | Val Loss: 6.6256 | Val PPL: 754.1485 | Val Acc: 0.0651\nEpoch 3/20 | Train Loss: 6.4067 | Val Loss: 6.5402 | Val PPL: 692.4570 | Val Acc: 0.0718\nEpoch 4/20 | Train Loss: 6.2403 | Val Loss: 6.4800 | Val PPL: 651.9438 | Val Acc: 0.0776\nEpoch 5/20 | Train Loss: 6.0844 | Val Loss: 6.4330 | Val PPL: 622.0229 | Val Acc: 0.0826\nEpoch 6/20 | Train Loss: 5.9364 | Val Loss: 6.3992 | Val PPL: 601.3865 | Val Acc: 0.0888\nEpoch 7/20 | Train Loss: 5.8016 | Val Loss: 6.3847 | Val PPL: 592.6930 | Val Acc: 0.0941\nEpoch 8/20 | Train Loss: 5.6756 | Val Loss: 6.3796 | Val PPL: 589.6842 | Val Acc: 0.0941\nEpoch 9/20 | Train Loss: 5.5554 | Val Loss: 6.3841 | Val PPL: 592.3700 | Val Acc: 0.0975\nEpoch 10/20 | Train Loss: 5.4375 | Val Loss: 6.3940 | Val PPL: 598.2458 | Val Acc: 0.0984\nEpoch 11/20 | Train Loss: 5.3320 | Val Loss: 6.3989 | Val PPL: 601.1782 | Val Acc: 0.0983\nEpoch 12/20 | Train Loss: 5.2295 | Val Loss: 6.4185 | Val PPL: 613.0758 | Val Acc: 0.1016\nEpoch 13/20 | Train Loss: 5.1287 | Val Loss: 6.4402 | Val PPL: 626.5627 | Val Acc: 0.1033\nEarly stopping triggered after 13 epochs.\nRunning EXP-FF-1024: feedforward_dim=1024 (Model=Transformer, Opt=Adam)\nEpoch 1/20 | Train Loss: 6.9681 | Val Loss: 6.6686 | Val PPL: 787.2696 | Val Acc: 0.0628\nEpoch 2/20 | Train Loss: 6.4978 | Val Loss: 6.5347 | Val PPL: 688.6051 | Val Acc: 0.0750\nEpoch 3/20 | Train Loss: 6.2819 | Val Loss: 6.4353 | Val PPL: 623.4415 | Val Acc: 0.0832\nEpoch 4/20 | Train Loss: 6.0925 | Val Loss: 6.3813 | Val PPL: 590.6873 | Val Acc: 0.0883\nEpoch 5/20 | Train Loss: 5.9257 | Val Loss: 6.3367 | Val PPL: 564.9387 | Val Acc: 0.0934\nEpoch 6/20 | Train Loss: 5.7746 | Val Loss: 6.3141 | Val PPL: 552.2934 | Val Acc: 0.0975\nEpoch 7/20 | Train Loss: 5.6346 | Val Loss: 6.3054 | Val PPL: 547.4990 | Val Acc: 0.0992\nEpoch 8/20 | Train Loss: 5.5069 | Val Loss: 6.2992 | Val PPL: 544.1167 | Val Acc: 0.1017\nEpoch 9/20 | Train Loss: 5.3854 | Val Loss: 6.3209 | Val PPL: 556.0962 | Val Acc: 0.1041\nEpoch 10/20 | Train Loss: 5.2674 | Val Loss: 6.3276 | Val PPL: 559.8155 | Val Acc: 0.1035\nEpoch 11/20 | Train Loss: 5.1526 | Val Loss: 6.3508 | Val PPL: 572.9703 | Val Acc: 0.1033\nEpoch 12/20 | Train Loss: 5.0446 | Val Loss: 6.3748 | Val PPL: 586.8597 | Val Acc: 0.1067\nEpoch 13/20 | Train Loss: 4.9408 | Val Loss: 6.3911 | Val PPL: 596.5208 | Val Acc: 0.1081\nEarly stopping triggered after 13 epochs.\nRunning EXP-BLOCKS-1: blocks=1 (Model=Transformer, Opt=Adam)\nEpoch 1/20 | Train Loss: 7.1505 | Val Loss: 6.7363 | Val PPL: 842.4755 | Val Acc: 0.0533\nEpoch 2/20 | Train Loss: 6.5611 | Val Loss: 6.6128 | Val PPL: 744.5819 | Val Acc: 0.0675\nEpoch 3/20 | Train Loss: 6.3442 | Val Loss: 6.5337 | Val PPL: 687.9183 | Val Acc: 0.0764\nEpoch 4/20 | Train Loss: 6.1644 | Val Loss: 6.4876 | Val PPL: 656.9674 | Val Acc: 0.0794\nEpoch 5/20 | Train Loss: 6.0093 | Val Loss: 6.4550 | Val PPL: 635.8902 | Val Acc: 0.0833\nEpoch 6/20 | Train Loss: 5.8627 | Val Loss: 6.4367 | Val PPL: 624.3289 | Val Acc: 0.0850\nEpoch 7/20 | Train Loss: 5.7343 | Val Loss: 6.4321 | Val PPL: 621.4651 | Val Acc: 0.0902\nEpoch 8/20 | Train Loss: 5.6130 | Val Loss: 6.4331 | Val PPL: 622.0725 | Val Acc: 0.0915\nEpoch 9/20 | Train Loss: 5.5006 | Val Loss: 6.4427 | Val PPL: 628.1131 | Val Acc: 0.0929\nEpoch 10/20 | Train Loss: 5.3997 | Val Loss: 6.4563 | Val PPL: 636.6939 | Val Acc: 0.0933\nEpoch 11/20 | Train Loss: 5.3032 | Val Loss: 6.4742 | Val PPL: 648.2251 | Val Acc: 0.0950\nEpoch 12/20 | Train Loss: 5.2156 | Val Loss: 6.4972 | Val PPL: 663.2511 | Val Acc: 0.0960\nEarly stopping triggered after 12 epochs.\nRunning EXP-BLOCKS-3: blocks=3 (Model=Transformer, Opt=Adam)\nEpoch 1/20 | Train Loss: 6.9818 | Val Loss: 6.6889 | Val PPL: 803.4197 | Val Acc: 0.0593\nEpoch 2/20 | Train Loss: 6.5391 | Val Loss: 6.5532 | Val PPL: 701.4866 | Val Acc: 0.0760\nEpoch 3/20 | Train Loss: 6.3389 | Val Loss: 6.4701 | Val PPL: 645.5418 | Val Acc: 0.0785\nEpoch 4/20 | Train Loss: 6.1634 | Val Loss: 6.4012 | Val PPL: 602.5490 | Val Acc: 0.0858\nEpoch 5/20 | Train Loss: 6.0039 | Val Loss: 6.3508 | Val PPL: 572.9500 | Val Acc: 0.0900\nEpoch 6/20 | Train Loss: 5.8537 | Val Loss: 6.3247 | Val PPL: 558.2083 | Val Acc: 0.0935\nEpoch 7/20 | Train Loss: 5.7148 | Val Loss: 6.3095 | Val PPL: 549.7874 | Val Acc: 0.0965\nEpoch 8/20 | Train Loss: 5.5800 | Val Loss: 6.3092 | Val PPL: 549.6294 | Val Acc: 0.0997\nEpoch 9/20 | Train Loss: 5.4560 | Val Loss: 6.3051 | Val PPL: 547.3806 | Val Acc: 0.1017\nEpoch 10/20 | Train Loss: 5.3315 | Val Loss: 6.3207 | Val PPL: 555.9499 | Val Acc: 0.1017\nEpoch 11/20 | Train Loss: 5.2132 | Val Loss: 6.3375 | Val PPL: 565.3792 | Val Acc: 0.1042\nEpoch 12/20 | Train Loss: 5.1037 | Val Loss: 6.3567 | Val PPL: 576.3173 | Val Acc: 0.1048\nEpoch 13/20 | Train Loss: 4.9920 | Val Loss: 6.3854 | Val PPL: 593.1418 | Val Acc: 0.1049\nEpoch 14/20 | Train Loss: 4.8841 | Val Loss: 6.4086 | Val PPL: 607.0442 | Val Acc: 0.1061\nEarly stopping triggered after 14 epochs.\nRunning EXP-BLOCKS-4: blocks=4 (Model=Transformer, Opt=Adam)\nEpoch 1/20 | Train Loss: 6.9728 | Val Loss: 6.6718 | Val PPL: 789.8097 | Val Acc: 0.0598\nEpoch 2/20 | Train Loss: 6.5248 | Val Loss: 6.5329 | Val PPL: 687.4006 | Val Acc: 0.0718\nEpoch 3/20 | Train Loss: 6.3272 | Val Loss: 6.4533 | Val PPL: 634.7651 | Val Acc: 0.0835\nEpoch 4/20 | Train Loss: 6.1569 | Val Loss: 6.3724 | Val PPL: 585.4833 | Val Acc: 0.0867\nEpoch 5/20 | Train Loss: 5.9988 | Val Loss: 6.3276 | Val PPL: 559.8135 | Val Acc: 0.0935\nEpoch 6/20 | Train Loss: 5.8501 | Val Loss: 6.2957 | Val PPL: 542.2285 | Val Acc: 0.0946\nEpoch 7/20 | Train Loss: 5.7093 | Val Loss: 6.2779 | Val PPL: 532.6777 | Val Acc: 0.1001\nEpoch 8/20 | Train Loss: 5.5788 | Val Loss: 6.2697 | Val PPL: 528.3282 | Val Acc: 0.1018\nEpoch 9/20 | Train Loss: 5.4464 | Val Loss: 6.2760 | Val PPL: 531.6529 | Val Acc: 0.1036\nEpoch 10/20 | Train Loss: 5.3233 | Val Loss: 6.2807 | Val PPL: 534.1610 | Val Acc: 0.1052\nEpoch 11/20 | Train Loss: 5.2032 | Val Loss: 6.3002 | Val PPL: 544.6866 | Val Acc: 0.1079\nEpoch 12/20 | Train Loss: 5.0871 | Val Loss: 6.3193 | Val PPL: 555.1689 | Val Acc: 0.1083\nEpoch 13/20 | Train Loss: 4.9675 | Val Loss: 6.3404 | Val PPL: 566.9994 | Val Acc: 0.1100\nEarly stopping triggered after 13 epochs.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Results and CSV Generation\ndf = pd.DataFrame(results_list)\n\n# Logic for 'Best?' column\n# We'll mark the best result in each Parameter group\n# First, ensure Perplexity is numeric\ndf['Perplexity'] = pd.to_numeric(df['Perplexity'])\n\n# Group by Parameter and find index of min Perplexity\n# Note: 'Baseline' has Parameter='-', so we treat it separately or include it in comparisons?\n# The user wants 'Best?' checkmark. Let's do it per parameter group.\n\nfor param in df['Parameter'].unique():\n    if param == '-': continue\n    \n    # Get subset\n    subset = df[df['Parameter'] == param]\n    if not subset.empty:\n        best_idx = subset['Perplexity'].idxmin()\n        # Compare with baseline? Usually we want to see if it beat baseline.\n        # But let's just mark the best in the group.\n        df.at[best_idx, 'Best?'] = ''\n\n# Mark Baseline as best if it's better than all variations? \n# Or just mark it as the reference. The image shows Baseline has a checkmark.\n# Let's assume Baseline is 'Best' until proven otherwise, or just mark it initially.\n# Actually, let's just mark the global best for each category if we can.\n# For simplicity matching the image:\ndf.loc[df['Experiment'] == 'Baseline', 'Best?'] = '' \n\n# Reorder columns\ncols = ['Experiment', 'Parameter', 'Value', 'Model', 'Optimizer', 'Perplexity', 'Change', 'Best?']\ndf = df[cols]\n\nprint(\"Hyperparameter Tuning Results:\")\ndisplay(df)\n\ndf.to_csv('hyperparameter_tuning_results.csv', index=False)\nprint(\"Saved to hyperparameter_tuning_results.csv\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:21:47.307688Z","iopub.execute_input":"2025-11-30T14:21:47.308296Z","iopub.status.idle":"2025-11-30T14:21:47.340343Z","shell.execute_reply.started":"2025-11-30T14:21:47.308270Z","shell.execute_reply":"2025-11-30T14:21:47.339532Z"}},"outputs":[{"name":"stdout","text":"Hyperparameter Tuning Results:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         Experiment        Parameter   Value        Model Optimizer  \\\n0          Baseline                -       -         LSTM   RMSprop   \n1      EXP-LAYERS-1       num_layers       1         LSTM   RMSprop   \n2      EXP-LAYERS-3       num_layers       3         LSTM   RMSprop   \n3   EXP-DROPOUT-0.1          dropout     0.1         LSTM   RMSprop   \n4   EXP-DROPOUT-0.3          dropout     0.3         LSTM   RMSprop   \n5   EXP-DROPOUT-0.5          dropout     0.5         LSTM   RMSprop   \n6     EXP-LR-0.0001    learning_rate  0.0001         LSTM   RMSprop   \n7       EXP-LR-0.01    learning_rate    0.01         LSTM   RMSprop   \n8      EXP-BATCH-32       batch_size      32         LSTM   RMSprop   \n9      EXP-BATCH-64       batch_size      64         LSTM   RMSprop   \n10    EXP-BATCH-256       batch_size     256         LSTM   RMSprop   \n11    EXP-EPOCHS-10           epochs      10         LSTM   RMSprop   \n12    EXP-EPOCHS-30           epochs      30         LSTM   RMSprop   \n13    EXP-EPOCHS-50           epochs      50         LSTM   RMSprop   \n14   Baseline-Trans                -       -  Transformer      Adam   \n15      EXP-HEADS-2        num_heads       2  Transformer      Adam   \n16      EXP-HEADS-8        num_heads       8  Transformer      Adam   \n17       EXP-FF-256  feedforward_dim     256  Transformer      Adam   \n18      EXP-FF-1024  feedforward_dim    1024  Transformer      Adam   \n19     EXP-BLOCKS-1           blocks       1  Transformer      Adam   \n20     EXP-BLOCKS-3           blocks       3  Transformer      Adam   \n21     EXP-BLOCKS-4           blocks       4  Transformer      Adam   \n\n    Perplexity  Change Best?  \n0       518.09       -       \n1       513.29     4.8       \n2       591.70  -73.61        \n3       523.74   -5.66       \n4       527.61   -9.52        \n5       531.29   -13.2        \n6       585.75  -67.66       \n7       980.48 -462.39        \n8       532.88   -14.8        \n9       525.57   -7.48       \n10      534.45  -16.36        \n11      519.14   -1.05        \n12      520.70   -2.61        \n13      515.37    2.72       \n14      569.75       -        \n15      569.65     0.1        \n16      566.84    2.91       \n17      589.68  -19.93        \n18      544.12   25.64       \n19      621.47  -51.71        \n20      547.38   22.37        \n21      528.33   41.43       ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Experiment</th>\n      <th>Parameter</th>\n      <th>Value</th>\n      <th>Model</th>\n      <th>Optimizer</th>\n      <th>Perplexity</th>\n      <th>Change</th>\n      <th>Best?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Baseline</td>\n      <td>-</td>\n      <td>-</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>518.09</td>\n      <td>-</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>EXP-LAYERS-1</td>\n      <td>num_layers</td>\n      <td>1</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>513.29</td>\n      <td>4.8</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EXP-LAYERS-3</td>\n      <td>num_layers</td>\n      <td>3</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>591.70</td>\n      <td>-73.61</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>EXP-DROPOUT-0.1</td>\n      <td>dropout</td>\n      <td>0.1</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>523.74</td>\n      <td>-5.66</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EXP-DROPOUT-0.3</td>\n      <td>dropout</td>\n      <td>0.3</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>527.61</td>\n      <td>-9.52</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>EXP-DROPOUT-0.5</td>\n      <td>dropout</td>\n      <td>0.5</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>531.29</td>\n      <td>-13.2</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>EXP-LR-0.0001</td>\n      <td>learning_rate</td>\n      <td>0.0001</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>585.75</td>\n      <td>-67.66</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>EXP-LR-0.01</td>\n      <td>learning_rate</td>\n      <td>0.01</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>980.48</td>\n      <td>-462.39</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>EXP-BATCH-32</td>\n      <td>batch_size</td>\n      <td>32</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>532.88</td>\n      <td>-14.8</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>EXP-BATCH-64</td>\n      <td>batch_size</td>\n      <td>64</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>525.57</td>\n      <td>-7.48</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>EXP-BATCH-256</td>\n      <td>batch_size</td>\n      <td>256</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>534.45</td>\n      <td>-16.36</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>EXP-EPOCHS-10</td>\n      <td>epochs</td>\n      <td>10</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>519.14</td>\n      <td>-1.05</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>EXP-EPOCHS-30</td>\n      <td>epochs</td>\n      <td>30</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>520.70</td>\n      <td>-2.61</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>EXP-EPOCHS-50</td>\n      <td>epochs</td>\n      <td>50</td>\n      <td>LSTM</td>\n      <td>RMSprop</td>\n      <td>515.37</td>\n      <td>2.72</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Baseline-Trans</td>\n      <td>-</td>\n      <td>-</td>\n      <td>Transformer</td>\n      <td>Adam</td>\n      <td>569.75</td>\n      <td>-</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>EXP-HEADS-2</td>\n      <td>num_heads</td>\n      <td>2</td>\n      <td>Transformer</td>\n      <td>Adam</td>\n      <td>569.65</td>\n      <td>0.1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>EXP-HEADS-8</td>\n      <td>num_heads</td>\n      <td>8</td>\n      <td>Transformer</td>\n      <td>Adam</td>\n      <td>566.84</td>\n      <td>2.91</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>EXP-FF-256</td>\n      <td>feedforward_dim</td>\n      <td>256</td>\n      <td>Transformer</td>\n      <td>Adam</td>\n      <td>589.68</td>\n      <td>-19.93</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>EXP-FF-1024</td>\n      <td>feedforward_dim</td>\n      <td>1024</td>\n      <td>Transformer</td>\n      <td>Adam</td>\n      <td>544.12</td>\n      <td>25.64</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>EXP-BLOCKS-1</td>\n      <td>blocks</td>\n      <td>1</td>\n      <td>Transformer</td>\n      <td>Adam</td>\n      <td>621.47</td>\n      <td>-51.71</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>EXP-BLOCKS-3</td>\n      <td>blocks</td>\n      <td>3</td>\n      <td>Transformer</td>\n      <td>Adam</td>\n      <td>547.38</td>\n      <td>22.37</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>EXP-BLOCKS-4</td>\n      <td>blocks</td>\n      <td>4</td>\n      <td>Transformer</td>\n      <td>Adam</td>\n      <td>528.33</td>\n      <td>41.43</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Saved to hyperparameter_tuning_results.csv\n","output_type":"stream"}],"execution_count":16}]}